
# DeepSeek Chatbot
A lightweight AI chatbot built with DeepSeek-R1-Distill-Qwen-1.5B model, designed for easy deployment and testingâ€”even on systems without GPU support.

âœ¨ Features
- Local & GPU-Free Inference â€“ Runs efficiently on CPU-only environments for testing.
- WebSocket Support â€“ Real-time interactive chat via WebSocket connections.
- Conversation Persistence â€“ Stores chat history in PostgreSQL for continuity.
- Dockerized Deployment â€“ Easy setup and scaling using Docker.
- REST API Endpoint â€“ Simple HTTP-based interaction for remote queries.

ğŸš€ Installation

My Environment:
- Windows 11 pro, CPU: Intel(R) Core(TM) i5-8365U CPU, RAM 16G
- VM in VMWare Workstation: 4C/8G/50G Ubuntu

Clone the repository:
git clone https://github.com/dalu810/deepseek-chatbot.git

Install dependencies:
(Include a requirements.txt or instructions for pip/poetry if applicable.)


ğŸ›  Usage

deepseek-chatbot/
â”‚
â”œâ”€â”€ chatbot/
â”‚   â”œâ”€â”€ chatbot.py
â”‚   â””â”€â”€ static/
â”œâ”€â”€ test/
â”‚   â”œâ”€â”€ deepseek_testing.py
â”‚   â”œâ”€â”€ deepseek_chatting.py
â”‚   â””â”€â”€ test.deepseek_remote.py
â””â”€â”€ websocket/
    â””â”€â”€ fastapi_websocket.py

In test folder:
#Local model testing with hardcoded queries
- deepseek_testing.py	

#Local model testing, interactive CLI chat
- deepseek_chatting.py	

#Remote: 
uvicorn deepseek_remote:app --host 0.0.0.0 --port 8000

#Test remotely, query example:
curl -X POST "http://<SERVER_IP>:8000/chat/" \
     -H "Content-Type: application/json" \
     -d '{"prompt": "What is AI?"}'
- deepseek_remote.py

In websocket folder
#Multiple users synchronization
server: uvicorn fastapi_websocket:app --host 0.0.0.0 --port 8000
client: websocat ws://localhost:8000/chat

In chatbot folder
#Chatbot with browser
server: uvicorn fastapi_websocket:app --host 0.0.0.0 --port 8000
client: http://<server IP>:8000/chatbot/static/chat.html


ğŸ“ Notes
The model DeepSeek-R1-Distill-Qwen-1.5B is optimized for low-resource environments, but it is slow without GPU support, just for the user who is interested in LLM usage locally.